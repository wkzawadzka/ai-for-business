{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing warehouse flows with Q-learning ###\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](case1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the enviorment\n",
    "_____________________________________\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States\n",
    "The set of environmental states S is defined as the finite set {s1, . . . , sN } where the size of the state space is N, i.e. |S| = N.\n",
    "\n",
    "- location at time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locations mapping:  {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11}\n",
      "\n",
      "\n",
      "states:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n"
     ]
    }
   ],
   "source": [
    "locations_encoding = {letter:i for i, letter in enumerate( \"ABCDEFGHIJKL\")} #locations_encoding\n",
    "print(\"locations mapping: \", locations_encoding)\n",
    "\n",
    "states = set(locations_encoding.values())\n",
    "print(\"\\n\\nstates: \", states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "The set of actions A is defined as the finite set {a1, . . . , aK} where the size of the action space is K, i.e. |A| = K. Actions can be used to control the system state. The set of actions that can be applied in some particular state s ∈ S, is denoted A(s), where A(s) ⊆ A.\n",
    "- set of neighbours of given location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors dictionary:  {'A': ['B'], 'B': ['A', 'F', 'C'], 'C': ['B', 'G'], 'D': ['H'], 'E': ['I'], 'F': ['B', 'J'], 'G': ['C', 'H'], 'H': ['D', 'G', 'L'], 'I': ['E', 'J'], 'J': ['F', 'I', 'K'], 'K': ['J', 'L'], 'L': ['K', 'H']}\n",
      "\n",
      "actions with letters:  [{'B'}, {'C', 'A', 'F'}, {'B', 'G'}, {'H'}, {'I'}, {'B', 'J'}, {'C', 'H'}, {'L', 'G', 'D'}, {'E', 'J'}, {'K', 'I', 'F'}, {'L', 'J'}, {'K', 'H'}]\n",
      "\n",
      "actions:  [{1}, {0, 2, 5}, {1, 6}, {7}, {8}, {1, 9}, {2, 7}, {11, 3, 6}, {9, 4}, {8, 10, 5}, {9, 11}, {10, 7}]\n"
     ]
    }
   ],
   "source": [
    "neighbors = {letter:[] for letter in \"ABCDEFGHIJKL\"} \n",
    "\n",
    "# populating neighbours\n",
    "neighbors[\"A\"].append(\"B\")\n",
    "neighbors[\"B\"].extend([\"A\", \"F\", \"C\"])\n",
    "neighbors[\"C\"].extend([\"B\", \"G\"])\n",
    "neighbors[\"D\"].append(\"H\")\n",
    "neighbors[\"E\"].append(\"I\")\n",
    "neighbors[\"F\"].extend([\"B\", \"J\"])\n",
    "neighbors[\"G\"].extend([\"C\", \"H\"])\n",
    "neighbors[\"H\"].extend([\"D\", \"G\", \"L\"])\n",
    "neighbors[\"I\"].extend([\"E\", \"J\"])\n",
    "neighbors[\"J\"].extend([\"F\", \"I\", \"K\"])\n",
    "neighbors[\"K\"].extend([\"J\", \"L\"])\n",
    "neighbors[\"L\"].extend([\"K\", \"H\"])\n",
    "print(\"neighbors dictionary: \", neighbors)\n",
    "\n",
    "actions = [set(neighbors[letter]) for letter in locations_encoding.keys()]\n",
    "print(\"\\nactions with letters: \", actions)\n",
    "actions = [set(locations_encoding[i] for i in neighbors[letter]) for letter in locations_encoding.keys()]\n",
    "print(\"\\nactions: \", actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- 2D matrix of position and move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards matrix:  [[   0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [   1.    0.    1.    0.    0.    1.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    1.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.]\n",
      " [   0.    1.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.]\n",
      " [   0.    0.    1.    0.    0.    0. 1000.    1.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    1.    0.    0.    1.    0.    0.    0.    0.    1.]\n",
      " [   0.    0.    0.    0.    1.    0.    0.    0.    0.    1.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    1.    0.    0.    1.    0.    1.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    1.]\n",
      " [   0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    1.    0.]]\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array(np.zeros([len(locations_encoding), len(locations_encoding)])) # initially 0 (move not possible)\n",
    "\n",
    "for i, location in enumerate(locations_encoding.keys()):\n",
    "    for neighbor in neighbors[location]:\n",
    "        rewards[i][locations_encoding[neighbor]] = 1\n",
    "\n",
    "# as we want to train algorithm to go to G, we will write 1000 at (G,G)\n",
    "rewards[locations_encoding[\"G\"]][locations_encoding[\"G\"]] = 1000\n",
    "\n",
    "print(\"rewards matrix: \", rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning solution\n",
    "_____________________________________\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](case1-pseudo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningSolver:\n",
    "    def __init__(self, states: set, actions: List[set], start_state:int, end_state:int, repetitions=1000, discount_factor = 0.75, learning_rate=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.repetitions = repetitions\n",
    "        self.gamma = discount_factor\n",
    "        self.lr = learning_rate\n",
    "        self.start = start_state\n",
    "        self.end = end_state\n",
    "        self.rewards = self.get_rewards()\n",
    "        self.qvalues = self.get_qvalues()\n",
    "    \n",
    "    def select_random_state(self):\n",
    "        return random.choice(tuple(self.states))\n",
    "    \n",
    "    def perform_random_action(self, curr: int):\n",
    "        return random.choice(tuple(self.actions[curr]))\n",
    "    \n",
    "    def get_rewards(self):\n",
    "        rewards = np.array(np.zeros([len(self.states), len(self.actions)]))\n",
    "        for state in self.states:\n",
    "            for next_state in self.actions[state]:\n",
    "                rewards[state][next_state] = 1\n",
    "        rewards[self.end][self.end] = 1000\n",
    "        self.actions[self.end].add(self.end)\n",
    "        return rewards\n",
    "            \n",
    "    def get_qvalues(self):\n",
    "        qvalues = np.array(np.zeros([len(self.states), len(self.actions)]))\n",
    "        for _ in range(self.repetitions):\n",
    "            # 1. Select random state\n",
    "            curr_state = self.select_random_state()\n",
    "            # 2. Play random action that can lead to next possible state\n",
    "            next_state = self.perform_random_action(curr_state)\n",
    "            # 3. We reach new state and get the reward\n",
    "            reward = self.rewards[curr_state][next_state]\n",
    "            # 4. We compute the Temporal Difference TD\n",
    "            td = reward + self.gamma * max([qvalues[next_state][a2] for a2 in self.actions[next_state]]) - qvalues[curr_state][next_state]\n",
    "            # 5. We update Q-value by applying the Bellmann equation\n",
    "            qvalues[curr_state][next_state] = qvalues[curr_state][next_state] + self.lr * td\n",
    "        return qvalues\n",
    "    \n",
    "    def get_route(self):\n",
    "        route = [self.start]\n",
    "        curr_state = self.start\n",
    "        while curr_state != self.end:\n",
    "            curr_state = np.argmax(self.qvalues[curr_state, ])\n",
    "            route.append(curr_state)\n",
    "        return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "\n",
      " [[   0    1    0    0    0    0    0    0    0    0    0    0]\n",
      " [   1    0    1    0    0    1    0    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    1    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    1    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0]\n",
      " [   0    1    0    0    0    0    0    0    0    1    0    0]\n",
      " [   0    0    1    0    0    0 1000    1    0    0    0    0]\n",
      " [   0    0    0    1    0    0    1    0    0    0    0    1]\n",
      " [   0    0    0    0    1    0    0    0    0    1    0    0]\n",
      " [   0    0    0    0    0    1    0    0    1    0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    1]\n",
      " [   0    0    0    0    0    0    0    1    0    0    1    0]]\n",
      "\n",
      "\n",
      "Q-valules:\n",
      "\n",
      " [[   0 1685    0    0    0    0    0    0    0    0    0    0]\n",
      " [1262    0 2246    0    0 1265    0    0    0    0    0    0]\n",
      " [   0 1685    0    0    0    0 2993    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 2248    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  712    0    0    0]\n",
      " [   0 1685    0    0    0    0    0    0    0  948    0    0]\n",
      " [   0    0 2246    0    0    0 3994 2245    0    0    0    0]\n",
      " [   0    0    0 1684    0    0 2997    0    0    0    0 1684]\n",
      " [   0    0    0    0  535    0    0    0    0  948    0    0]\n",
      " [   0    0    0    0    0 1265    0    0  711    0 1257    0]\n",
      " [   0    0    0    0    0    0    0    0    0  948    0 1684]\n",
      " [   0    0    0    0    0    0    0 2248    0    0 1263    0]]\n"
     ]
    }
   ],
   "source": [
    "states = set(locations_encoding.values())\n",
    "actions = [set(locations_encoding[i] for i in neighbors[letter]) for letter in locations_encoding.keys()]\n",
    "solver = QLearningSolver(states, actions, locations_encoding[\"E\"], locations_encoding[\"G\"])\n",
    "\n",
    "print(\"Rewards:\\n\\n\", solver.rewards.astype(int))\n",
    "print(\"\\n\\nQ-valules:\\n\\n\", solver.qvalues.astype(int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going into production\n",
    "_____________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'I', 'J', 'K', 'L', 'H', 'G']\n"
     ]
    }
   ],
   "source": [
    "solver = QLearningSolver(states, actions, locations_encoding[\"E\"], locations_encoding[\"G\"])\n",
    "states_mapping = {val:key for key, val in locations_encoding.items()}\n",
    "route = solver.get_route()\n",
    "print([states_mapping[state] for state in route])\n",
    "# ['E', 'I', 'J', 'F', 'B', 'C', 'G'] or ['E', 'I', 'J', 'K', 'L', 'H', 'G']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediary location\n",
    "_____________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3 solution (quick & better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'I', 'J', 'K', 'L', 'H', 'G']\n"
     ]
    }
   ],
   "source": [
    "route = QLearningSolver(states, actions, locations_encoding[\"E\"], locations_encoding[\"K\"]).get_route() + QLearningSolver(states, actions, locations_encoding[\"K\"], locations_encoding[\"G\"]).get_route()\n",
    "del route[int(len(route)/2)] # remove repeted letter\n",
    "print([states_mapping[state] for state in route])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "#### Other solution (worse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningSolverWithIntermediary(QLearningSolver):\n",
    "    def __init__(self, states: set, actions: List[set], start_state: int, end_state: int, intermediary_from: int, intermediary_to: int, repetitions=1000, discount_factor=0.75, learning_rate=0.9):\n",
    "        self.inter_start = intermediary_from\n",
    "        self.inter_end = intermediary_to\n",
    "        super().__init__(states, actions, start_state, end_state, repetitions, discount_factor, learning_rate)\n",
    "\n",
    "    def get_rewards(self):\n",
    "        super().get_rewards()\n",
    "        rewards[self.inter_start][self.inter_end] = 500\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "\n",
      " [[   0    1    0    0    0    0    0    0    0    0    0    0]\n",
      " [   1    0    1    0    0    1    0    0    0    0    0    0]\n",
      " [   0    1    0    0    0    0    1    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    1    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    1    0    0    0]\n",
      " [   0    1    0    0    0    0    0    0    0    1    0    0]\n",
      " [   0    0    1    0    0    0 1000    1    0    0    0    0]\n",
      " [   0    0    0    1    0    0    1    0    0    0    0    1]\n",
      " [   0    0    0    0    1    0    0    0    0    1    0    0]\n",
      " [   0    0    0    0    0    1    0    0    1    0  500    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    1]\n",
      " [   0    0    0    0    0    0    0    1    0    0    1    0]]\n",
      "\n",
      "\n",
      "Q-valules:\n",
      "\n",
      " [[   0 1689    0    0    0    0    0    0    0    0    0    0]\n",
      " [1267    0 2251    0    0 1266    0    0    0    0    0    0]\n",
      " [   0 1689    0    0    0    0 3000    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 2250    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  995    0    0    0]\n",
      " [   0 1689    0    0    0    0    0    0    0 1326    0    0]\n",
      " [   0    0 2250    0    0    0 3998 2251    0    0    0    0]\n",
      " [   0    0    0 1689    0    0 3000    0    0    0    0 1688]\n",
      " [   0    0    0    0  747    0    0    0    0 1325    0    0]\n",
      " [   0    0    0    0    0 1267    0    0  990    0 1766    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1326    0 1689]\n",
      " [   0    0    0    0    0    0    0 2250    0    0 1267    0]]\n"
     ]
    }
   ],
   "source": [
    "states = set(locations_encoding.values())\n",
    "actions = [set(locations_encoding[i] for i in neighbors[letter]) for letter in locations_encoding.keys()]\n",
    "solver2 = QLearningSolverWithIntermediary(states, actions, locations_encoding[\"E\"], locations_encoding[\"G\"], locations_encoding[\"J\"], locations_encoding[\"K\"])\n",
    "\n",
    "print(\"Rewards:\\n\\n\", solver2.rewards.astype(int))\n",
    "print(\"\\n\\nQ-valules:\\n\\n\", solver2.qvalues.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'I', 'J', 'K', 'L', 'H', 'G']\n"
     ]
    }
   ],
   "source": [
    "solver2 = QLearningSolverWithIntermediary(states, actions, locations_encoding[\"E\"], locations_encoding[\"G\"], locations_encoding[\"J\"], locations_encoding[\"K\"])\n",
    "route = solver2.get_route()\n",
    "print([states_mapping[state] for state in route]) # note: always the same route through K: ['E', 'I', 'J', 'K', 'L', 'H', 'G']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Markov Decision Processes: Concepts and Algorithms, Martijn van Otterlo, May 2009\n",
    "\n",
    "2. Artificial Intelligence for Business Udemy Course  [Link to the course](https://www.udemy.com/share/101Y7E3@ufylekA5UWd1-mxWKBL2ecxPn01pSdl5tfu-wGkkbyEcOpnoF9HyJXi_3vey6XqgmQ==/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5bf1b1c2666f930d89b0cedc37bc172e36184b5884258e5d17d5b824b37ab44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
